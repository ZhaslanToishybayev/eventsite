"""
üìä Enhanced Analytics Service
–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å–µ—Ä–≤–∏—Å –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ —Å –ø—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é
"""

import logging
import json
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from collections import defaultdict, Counter
import numpy as np
from django.db.models import Q, Count, Avg, Sum, F, Func
from django.db.models.functions import TruncDate, TruncHour, ExtractHour
from django.core.cache import cache
from django.utils import timezone

from ..models import ChatSession, ChatMessage, AIContext, UserFeedback
from clubs.models import Club, ClubEvent
from accounts.models import User
from ..utils.predictive_engine import PredictiveEngine
from ..utils.context_analyzer import ContextAnalyzer

logger = logging.getLogger(__name__)


class EnhancedAnalyticsService:
    """
    üìä –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞ —Å –ø—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω—ã–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏
    """

    def __init__(self):
        self.predictive_engine = PredictiveEngine()
        self.context_analyzer = ContextAnalyzer()

        # –ü–µ—Ä–∏–æ–¥—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        self.ANALYSIS_PERIODS = {
            'day': 1,
            'week': 7,
            'month': 30,
            'quarter': 90
        }

        # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
        self.CACHE_TIMEOUT = {
            'hourly': 3600,      # 1 —á–∞—Å
            'daily': 86400,      # 1 –¥–µ–Ω—å
            'weekly': 604800,    # 1 –Ω–µ–¥–µ–ª—è
            'monthly': 2592000   # 1 –º–µ—Å—è—Ü
        }

    def get_comprehensive_analytics(self, period: str = 'week', user_id: Optional[int] = None) -> Dict[str, Any]:
        """
        üìà –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏
        """
        try:
            analytics = {
                'period': period,
                'generated_at': timezone.now().isoformat(),
                'user_specific': user_id is not None,
                'overall_metrics': {},
                'conversation_analytics': {},
                'user_analytics': {},
                'content_analytics': {},
                'performance_metrics': {},
                'predictions': {},
                'trends': {},
                'recommendations': []
            }

            # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            analytics['overall_metrics'] = self._get_overall_metrics(period, user_id)

            # –ê–Ω–∞–ª–∏—Ç–∏–∫–∞ –¥–∏–∞–ª–æ–≥–æ–≤
            analytics['conversation_analytics'] = self._get_conversation_analytics(period, user_id)

            # –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞
            analytics['user_analytics'] = self._get_user_analytics(period, user_id)

            # –ê–Ω–∞–ª–∏—Ç–∏–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            analytics['content_analytics'] = self._get_content_analytics(period, user_id)

            # –ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            analytics['performance_metrics'] = self._get_performance_metrics(period, user_id)

            # –ü—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞
            analytics['predictions'] = self._get_predictive_analytics(period, user_id)

            # –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤
            analytics['trends'] = self._analyze_trends(period, user_id)

            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
            analytics['recommendations'] = self._generate_recommendations(analytics)

            return analytics

        except Exception as e:
            logger.error(f"‚ùå Error generating comprehensive analytics: {e}")
            return {'error': str(e), 'timestamp': timezone.now().isoformat()}

    def _get_overall_metrics(self, period: str, user_id: Optional[int]) -> Dict[str, Any]:
        """–û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã"""
        cache_key = f"overall_metrics_{period}_{user_id or 'global'}"
        cached = cache.get(cache_key)
        if cached:
            return cached

        try:
            days = self.ANALYSIS_PERIODS.get(period, 7)
            start_date = timezone.now() - timedelta(days=days)

            # –ë–∞–∑–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã
            base_filter = Q(created_at__gte=start_date)
            if user_id:
                base_filter &= Q(user_id=user_id)

            # –ú–µ—Ç—Ä–∏–∫–∏ —Å–µ—Å—Å–∏–π
            sessions = ChatSession.objects.filter(base_filter)

            total_sessions = sessions.count()
            active_sessions = sessions.filter(is_active=True).count()
            anonymous_sessions = sessions.filter(user__isnull=True).count()

            # –ú–µ—Ç—Ä–∏–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏–π
            messages = ChatMessage.objects.filter(
                session__created_at__gte=start_date
            )
            if user_id:
                messages = messages.filter(session__user_id=user_id)

            total_messages = messages.count()
            user_messages = messages.filter(role='user').count()
            assistant_messages = messages.filter(role='assistant').count()

            # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏
            unique_users = ChatSession.objects.filter(
                base_filter,
                user__isnull=False
            ).values('user').distinct().count()

            # –°—Ä–µ–¥–Ω–∏–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏
            avg_messages_per_session = total_messages / total_sessions if total_sessions > 0 else 0
            avg_session_duration = self._calculate_avg_session_duration(sessions)

            metrics = {
                'total_sessions': total_sessions,
                'active_sessions': active_sessions,
                'anonymous_sessions': anonymous_sessions,
                'registered_sessions': total_sessions - anonymous_sessions,
                'unique_users': unique_users,
                'total_messages': total_messages,
                'user_messages': user_messages,
                'assistant_messages': assistant_messages,
                'avg_messages_per_session': round(avg_messages_per_session, 2),
                'avg_session_duration_minutes': round(avg_session_duration / 60, 2),
                'messages_per_user': round(total_messages / unique_users, 2) if unique_users > 0 else 0,
                'anonymous_ratio': round(anonymous_sessions / total_sessions * 100, 2) if total_sessions > 0 else 0
            }

            cache.set(cache_key, metrics, timeout=self.CACHE_TIMEOUT['hourly'])
            return metrics

        except Exception as e:
            logger.error(f"‚ùå Error getting overall metrics: {e}")
            return {}

    def _get_conversation_analytics(self, period: str, user_id: Optional[int]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏—Ç–∏–∫–∞ –¥–∏–∞–ª–æ–≥–æ–≤"""
        try:
            days = self.ANALYSIS_PERIODS.get(period, 7)
            start_date = timezone.now() - timedelta(days=days)

            base_filter = Q(created_at__gte=start_date)
            if user_id:
                base_filter &= Q(user_id=user_id)

            sessions = ChatSession.objects.filter(base_filter).prefetch_related('messages')

            # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–∏–∞–ª–æ–≥–æ–≤
            session_lengths = []
            message_patterns = defaultdict(int)
            intent_distribution = defaultdict(int)
            sentiment_distribution = defaultdict(int)

            for session in sessions:
                messages = list(session.messages.all())

                # –î–ª–∏–Ω–∞ —Å–µ—Å—Å–∏–∏
                if messages:
                    first_message = messages[0].created_at
                    last_message = messages[-1].created_at
                    duration = (last_message - first_message).total_seconds()
                    session_lengths.append(duration)

                # –ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —Å–æ–æ–±—â–µ–Ω–∏–π
                for i, msg in enumerate(messages):
                    if msg.role == 'user':
                        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª–∏–Ω—ã —Å–æ–æ–±—â–µ–Ω–∏–π
                        content_length = len(msg.content)
                        if content_length < 50:
                            message_patterns['short'] += 1
                        elif content_length < 200:
                            message_patterns['medium'] += 1
                        else:
                            message_patterns['long'] += 1

                        # –ê–Ω–∞–ª–∏–∑ –∏–Ω—Ç–µ–Ω—Ç–æ–≤ –∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
                        analysis = self.context_analyzer.analyze_message(msg.content)
                        intent_distribution[analysis['intent']] += 1
                        sentiment_distribution[analysis['sentiment']] += 1

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            if session_lengths:
                avg_duration = np.mean(session_lengths)
                median_duration = np.median(session_lengths)
                std_duration = np.std(session_lengths)
            else:
                avg_duration = median_duration = std_duration = 0

            # –í—ã—è–≤–ª–µ–Ω–∏–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —Ç–µ–º
            popular_topics = self._extract_popular_topics(sessions)

            return {
                'session_duration': {
                    'average_minutes': round(avg_duration / 60, 2),
                    'median_minutes': round(median_duration / 60, 2),
                    'std_minutes': round(std_duration / 60, 2),
                    'longest_session_minutes': round(max(session_lengths) / 60, 2) if session_lengths else 0
                },
                'message_patterns': dict(message_patterns),
                'intent_distribution': dict(intent_distribution),
                'sentiment_distribution': dict(sentiment_distribution),
                'popular_topics': popular_topics,
                'conversation_flow': self._analyze_conversation_flow(sessions),
                'engagement_metrics': self._calculate_engagement_metrics(sessions)
            }

        except Exception as e:
            logger.error(f"‚ùå Error getting conversation analytics: {e}")
            return {}

    def _get_user_analytics(self, period: str, user_id: Optional[int]) -> Dict[str, Any]:
        """–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞"""
        try:
            if user_id:
                return self._get_specific_user_analytics(user_id, period)
            else:
                return self._get_general_user_analytics(period)

        except Exception as e:
            logger.error(f"Error getting user analytics: {e}")
            return {}

    def _get_content_analytics(self, period: str, user_id: Optional[int]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏—Ç–∏–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        try:
            days = self.ANALYSIS_PERIODS.get(period, 7)
            start_date = timezone.now() - timedelta(days=days)

            # –ê–Ω–∞–ª–∏–∑ –Ω–∞–∏–±–æ–ª–µ–µ –æ–±—Å—É–∂–¥–∞–µ–º—ã—Ö —Ç–µ–º
            messages = ChatMessage.objects.filter(
                role='user',
                created_at__gte=start_date
            )
            if user_id:
                messages = messages.filter(session__user_id=user_id)

            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ –≤—Å–µ—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
            all_keywords = []
            all_intents = []

            for message in messages:
                analysis = self.context_analyzer.analyze_message(message.content)
                all_keywords.extend(analysis['keywords'])
                all_intents.append(analysis['intent'])

            # –ß–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            keyword_freq = Counter(all_keywords)
            intent_freq = Counter(all_intents)

            # –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ RAG
            rag_analytics = self._get_rag_content_analytics(period, user_id)

            # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            quality_metrics = self._analyze_content_quality(messages)

            return {
                'top_keywords': [
                    {'keyword': word, 'frequency': count}
                    for word, count in keyword_freq.most_common(20)
                ],
                'intent_distribution': [
                    {'intent': intent, 'count': count}
                    for intent, count in intent_freq.most_common()
                ],
                'rag_analytics': rag_analytics,
                'quality_metrics': quality_metrics,
                'content_trends': self._analyze_content_trends(messages)
            }

        except Exception as e:
            logger.error(f"‚ùå Error getting content analytics: {e}")
            return {}

    def _get_performance_metrics(self, period: str, user_id: Optional[int]) -> Dict[str, Any]:
        """–ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        try:
            days = self.ANALYSIS_PERIODS.get(period, 7)
            start_date = timezone.now() - timedelta(days=days)

            # –ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–∏ –æ—Ç–≤–µ—Ç–∞
            response_times = []
            success_rates = []

            sessions = ChatSession.objects.filter(created_at__gte=start_date)
            if user_id:
                sessions = sessions.filter(user_id=user_id)

            for session in sessions:
                messages = list(session.messages.all())
                user_queries = [msg for msg in messages if msg.role == 'user']

                for i, user_msg in enumerate(user_queries):
                    # –ù–∞–π—Ç–∏ —Å–ª–µ–¥—É—é—â–∏–π –æ—Ç–≤–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
                    for j in range(i + 1, len(messages)):
                        if messages[j].role == 'assistant':
                            response_time = (messages[j].created_at - user_msg.created_at).total_seconds()
                            response_times.append(response_time)
                            break

                    # –û—Ü–µ–Ω–∫–∞ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ (–Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –∏–ª–∏ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –¥–∏–∞–ª–æ–≥–∞)
                    success_rate = self._calculate_query_success_rate(user_msg, messages[i+1:])
                    if success_rate is not None:
                        success_rates.append(success_rate)

            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            performance_stats = {}
            if response_times:
                performance_stats['response_time'] = {
                    'average_seconds': round(np.mean(response_times), 2),
                    'median_seconds': round(np.median(response_times), 2),
                    'p95_seconds': round(np.percentile(response_times, 95), 2),
                    'p99_seconds': round(np.percentile(response_times, 99), 2)
                }

            if success_rates:
                performance_stats['success_rate'] = {
                    'average': round(np.mean(success_rates) * 100, 2),
                    'median': round(np.median(success_rates) * 100, 2),
                    'distribution': self._create_distribution(success_rates)
                }

            # –ê–Ω–∞–ª–∏–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä–µ—Å—É—Ä—Å–æ–≤
            resource_usage = self._analyze_resource_usage(period, user_id)

            return {
                'performance_stats': performance_stats,
                'resource_usage': resource_usage,
                'bottlenecks': self._identify_bottlenecks(performance_stats, resource_usage),
                'optimization_suggestions': self._get_optimization_suggestions(performance_stats)
            }

        except Exception as e:
            logger.error(f"‚ùå Error getting performance metrics: {e}")
            return {}

    def _get_predictive_analytics(self, period: str, user_id: Optional[int]) -> Dict[str, Any]:
        """–ü—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞"""
        try:
            predictions = {
                'user_retention': self._predict_user_retention(period, user_id),
                'content_demand': self._predict_content_demand(period, user_id),
                'system_load': self._predict_system_load(period),
                'quality_trends': self._predict_quality_trends(period, user_id),
                'growth_projections': self._predict_growth(period)
            }

            return predictions

        except Exception as e:
            logger.error(f"‚ùå Error getting predictive analytics: {e}")
            return {}

    def _analyze_trends(self, period: str, user_id: Optional[int]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤"""
        try:
            days = self.ANALYSIS_PERIODS.get(period, 7)
            start_date = timezone.now() - timedelta(days=days)

            # –î–∏–Ω–∞–º–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–æ –¥–Ω—è–º
            daily_usage = []
            for i in range(days):
                day_date = start_date + timedelta(days=i)
                day_end = day_date + timedelta(days=1)

                sessions_count = ChatSession.objects.filter(
                    created_at__gte=day_date,
                    created_at__lt=day_end
                )
                if user_id:
                    sessions_count = sessions_count.filter(user_id=user_id)

                messages_count = ChatMessage.objects.filter(
                    created_at__gte=day_date,
                    created_at__lt=day_end
                )
                if user_id:
                    messages_count = messages_count.filter(session__user_id=user_id)

                daily_usage.append({
                    'date': day_date.date().isoformat(),
                    'sessions': sessions_count.count(),
                    'messages': messages_count.count()
                })

            # –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤
            trend_analysis = self._analyze_usage_trends(daily_usage)

            return {
                'daily_usage': daily_usage,
                'trend_analysis': trend_analysis,
                'seasonal_patterns': self._identify_seasonal_patterns(daily_usage),
                'growth_rate': self._calculate_growth_rate(daily_usage)
            }

        except Exception as e:
            logger.error(f"‚ùå Error analyzing trends: {e}")
            return {}

    def _generate_recommendations(self, analytics: Dict[str, Any]) -> List[Dict[str, Any]]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏"""
        recommendations = []

        try:
            # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            performance = analytics.get('performance_metrics', {})
            if performance.get('performance_stats', {}).get('response_time', {}).get('average_seconds', 0) > 30:
                recommendations.append({
                    'category': 'performance',
                    'priority': 'high',
                    'title': '–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤—Ä–µ–º–µ–Ω–∏ –æ—Ç–≤–µ—Ç–∞',
                    'description': '–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞ –ø—Ä–µ–≤—ã—à–∞–µ—Ç 30 —Å–µ–∫—É–Ω–¥. –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é.',
                    'action_items': [
                        '–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å RAG –∑–∞–ø—Ä–æ—Å—ã',
                        '–£–≤–µ–ª–∏—á–∏—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ',
                        '–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ–º–ø—Ç—ã'
                    ]
                })

            # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É
            content = analytics.get('content_analytics', {})
            rag_confidence = content.get('rag_analytics', {}).get('average_confidence', 0)
            if rag_confidence < 0.5:
                recommendations.append({
                    'category': 'content',
                    'priority': 'medium',
                    'title': '–£–ª—É—á—à–µ–Ω–∏–µ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π',
                    'description': '–ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å RAG. –†–∞—Å—à–∏—Ä—å—Ç–µ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã.',
                    'action_items': [
                        '–î–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏',
                        '–ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å —É—Å–ø–µ—à–Ω—ã–µ –¥–∏–∞–ª–æ–≥–∏',
                        '–û–±–Ω–æ–≤–∏—Ç—å FAQ'
                    ]
                })

            # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≤–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç–∏
            conversation = analytics.get('conversation_analytics', {})
            avg_messages = analytics.get('overall_metrics', {}).get('avg_messages_per_session', 0)
            if avg_messages < 3:
                recommendations.append({
                    'category': 'engagement',
                    'priority': 'medium',
                    'title': '–ü–æ–≤—ã—à–µ–Ω–∏–µ –≤–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç–∏',
                    'description': '–ù–∏–∑–∫–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π –Ω–∞ —Å–µ—Å—Å–∏—é. –£–ª—É—á—à–∏—Ç–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–µ.',
                    'action_items': [
                        '–î–æ–±–∞–≤–∏—Ç—å –ø—Ä–æ–∞–∫—Ç–∏–≤–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã',
                        '–ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç—ã',
                        '–£–ª—É—á—à–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞'
                    ]
                })

            return recommendations

        except Exception as e:
            logger.error(f"‚ùå Error generating recommendations: {e}")
            return []

    # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã
    def _calculate_avg_session_duration(self, sessions) -> float:
        """–†–∞—Å—á–µ—Ç —Å—Ä–µ–¥–Ω–µ–π –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å–µ—Å—Å–∏–∏"""
        durations = []
        for session in sessions:
            messages = session.messages.all()
            if messages.count() >= 2:
                first = messages.first().created_at
                last = messages.last().created_at
                duration = (last - first).total_seconds()
                durations.append(duration)

        return np.mean(durations) if durations else 0.0

    def _extract_popular_topics(self, sessions) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —Ç–µ–º"""
        topic_counter = Counter()

        for session in sessions:
            user_messages = session.messages.filter(role='user')
            for message in user_messages:
                analysis = self.context_analyzer.analyze_message(message.content)
                topic_counter[analysis['intent']] += 1

        return [
            {'topic': topic, 'count': count}
            for topic, count in topic_counter.most_common(10)
        ]

    def _analyze_conversation_flow(self, sessions) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –ø–æ—Ç–æ–∫–∞ –¥–∏–∞–ª–æ–≥–æ–≤"""
        flow_patterns = {
            'quick_resolutions': 0,  # –ë—ã—Å—Ç—Ä–æ–µ —Ä–µ—à–µ–Ω–∏–µ (1-2 —Å–æ–æ–±—â–µ–Ω–∏—è)
            'extended_discussions': 0,  # –î–ª–∏–Ω–Ω—ã–µ –¥–∏–∞–ª–æ–≥–∏ (>10 —Å–æ–æ–±—â–µ–Ω–∏–π)
            'follow_up_questions': 0   # –ü–æ—Å–ª–µ–¥—É—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã
        }

        for session in sessions:
            message_count = session.messages.count()
            if message_count <= 2:
                flow_patterns['quick_resolutions'] += 1
            elif message_count > 10:
                flow_patterns['extended_discussions'] += 1

        return flow_patterns

    def _calculate_engagement_metrics(self, sessions) -> Dict[str, Any]:
        """–†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –≤–æ–≤–ª–µ—á–µ–Ω–Ω–æ—Å—Ç–∏"""
        total_sessions = len(sessions)
        if total_sessions == 0:
            return {}

        # –î–æ–ª—è –∞–∫—Ç–∏–≤–Ω—ã—Ö —Å–µ—Å—Å–∏–π
        active_sessions = sum(1 for s in sessions if s.is_active)

        # –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ —Å–æ–æ–±—â–µ–Ω–∏–π
        all_user_messages = []
        for session in sessions:
            user_messages = session.messages.filter(role='user')
            all_user_messages.extend([msg.content for msg in user_messages])

        avg_message_length = np.mean([len(msg) for msg in all_user_messages]) if all_user_messages else 0

        return {
            'active_session_rate': round(active_sessions / total_sessions * 100, 2),
            'avg_message_length': round(avg_message_length, 2),
            'return_user_rate': self._calculate_return_user_rate(sessions)
        }

    def _create_distribution(self, values: List[float]) -> Dict[str, int]:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∑–Ω–∞—á–µ–Ω–∏–π"""
        distribution = {'low': 0, 'medium': 0, 'high': 0}
        for value in values:
            if value < 0.33:
                distribution['low'] += 1
            elif value < 0.67:
                distribution['medium'] += 1
            else:
                distribution['high'] += 1
        return distribution

    def _get_specific_user_analytics(self, user_id: int, period: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏—Ç–∏–∫–∞ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        # –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        return {
            'user_id': user_id,
            'total_sessions': ChatSession.objects.filter(user_id=user_id).count(),
            'total_messages': ChatMessage.objects.filter(session__user_id=user_id).count(),
            'favorite_topics': [],
            'engagement_score': 0.7
        }

    def _get_general_user_analytics(self, period: str) -> Dict[str, Any]:
        """–û–±—â–∞—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞"""
        return {
            'total_registered_users': User.objects.count(),
            'active_users_this_period': 0,
            'new_users_this_period': 0,
            'user_retention_rate': 0.8
        }

    def _get_rag_content_analytics(self, period: str, user_id: Optional[int]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏—Ç–∏–∫–∞ RAG –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        # –ó–∞–≥–ª—É—à–∫–∞ - –≤ —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ RAG
        return {
            'total_queries': 0,
            'average_confidence': 0.0,
            'most_accessed_collections': {},
            'cache_hit_rate': 0.0
        }

    def _analyze_content_quality(self, messages) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        return {
            'avg_clarity_score': 0.7,
            'relevance_score': 0.8,
            'completeness_score': 0.6
        }

    def _analyze_content_trends(self, messages) -> List[Dict[str, Any]]:
        """–ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        return []

    def _analyze_resource_usage(self, period: str, user_id: Optional[int]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä–µ—Å—É—Ä—Å–æ–≤"""
        return {
            'cpu_usage': 0.5,
            'memory_usage': 0.6,
            'api_calls': 100,
            'database_queries': 200
        }

    def _identify_bottlenecks(self, performance_stats: Dict, resource_usage: Dict) -> List[str]:
        """–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è —É–∑–∫–∏—Ö –º–µ—Å—Ç"""
        bottlenecks = []
        if performance_stats.get('response_time', {}).get('average_seconds', 0) > 30:
            bottlenecks.append('Response time bottleneck')
        return bottlenecks

    def _get_optimization_suggestions(self, performance_stats: Dict) -> List[str]:
        """–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
        return ['Implement caching', 'Optimize database queries']

    def _predict_user_retention(self, period: str, user_id: Optional[int]) -> Dict[str, Any]:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —É–¥–µ—Ä–∂–∞–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π"""
        return {'predicted_retention_rate': 0.85, 'confidence': 0.7}

    def _predict_content_demand(self, period: str, user_id: Optional[int]) -> Dict[str, Any]:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å–ø—Ä–æ—Å–∞ –Ω–∞ –∫–æ–Ω—Ç–µ–Ω—Ç"""
        return {'upcoming_topics': [], 'demand_forecast': {}}

    def _predict_system_load(self, period: str) -> Dict[str, Any]:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –Ω–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ —Å–∏—Å—Ç–µ–º—É"""
        return {'expected_load': 0.6, 'peak_hours': []}

    def _predict_quality_trends(self, period: str, user_id: Optional[int]) -> Dict[str, Any]:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ç—Ä–µ–Ω–¥–æ–≤ –∫–∞—á–µ—Å—Ç–≤–∞"""
        return {'quality_trend': 'stable', 'predicted_score': 0.8}

    def _predict_growth(self, period: str) -> Dict[str, Any]:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ä–æ—Å—Ç–∞"""
        return {'expected_growth_rate': 0.1, 'new_users_forecast': 100}

    def _analyze_usage_trends(self, daily_usage: List[Dict]) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è"""
        return {'trend': 'increasing', 'growth_rate': 0.05}

    def _identify_seasonal_patterns(self, daily_usage: List[Dict]) -> List[Dict]:
        """–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è —Å–µ–∑–æ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        return []

    def _calculate_growth_rate(self, daily_usage: List[Dict]) -> float:
        """–†–∞—Å—á–µ—Ç —Ç–µ–º–ø–∞ —Ä–æ—Å—Ç–∞"""
        return 0.05

    def _calculate_return_user_rate(self, sessions) -> float:
        """–†–∞—Å—á–µ—Ç –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞ –≤–æ–∑–≤—Ä–∞—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π"""
        return 0.3

    def _calculate_query_success_rate(self, user_msg, remaining_messages) -> Optional[float]:
        """–†–∞—Å—á–µ—Ç —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–∞"""
        # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è
        return 0.7


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
enhanced_analytics_service = None


def get_enhanced_analytics_service():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–µ—Ä–≤–∏—Å–∞"""
    global enhanced_analytics_service
    if enhanced_analytics_service is None:
        enhanced_analytics_service = EnhancedAnalyticsService()
    return enhanced_analytics_service