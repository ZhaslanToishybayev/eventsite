"""
ü§ñ OpenAI –∫–ª–∏–µ–Ω—Ç —Å–µ—Ä–≤–∏—Å v2.0
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ä–∞–±–æ—Ç–∞ —Å OpenAI API
"""

import logging
from typing import Dict, Any, Optional, List
from django.conf import settings
from openai import OpenAI
from django.core.cache import cache
import json

from .base import BaseAIService


class OpenAIClientService(BaseAIService):
    """
    –°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å OpenAI API
    """

    def __init__(self):
        super().__init__()
        self.client = None
        self.model = getattr(settings, 'OPENAI_MODEL', 'gpt-3.5-turbo')
        self.max_tokens = getattr(settings, 'OPENAI_MAX_TOKENS', 1000)
        self.temperature = getattr(settings, 'OPENAI_TEMPERATURE', 0.7)
        self.timeout = getattr(settings, 'OPENAI_TIMEOUT', 30)
        self._initialize_client()

    def _initialize_client(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è OpenAI –∫–ª–∏–µ–Ω—Ç–∞"""
        try:
            api_key = getattr(settings, 'OPENAI_API_KEY', None)
            if not api_key:
                self.log_error("OpenAI API –∫–ª—é—á –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
                return

            self.client = OpenAI(api_key=api_key)
            self.log_info("OpenAI –∫–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

        except Exception as e:
            self.log_error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ OpenAI –∫–ª–∏–µ–Ω—Ç–∞: {e}")
            self.client = None

    def process(self, messages: List[Dict[str, str]], **kwargs) -> Dict[str, Any]:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏–π
        """
        return self.chat_completion(messages, **kwargs)

    def chat_completion(self, messages: List[Dict[str, str]], **kwargs) -> Dict[str, Any]:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –∑–∞–ø—Ä–æ—Å –∫ OpenAI Chat Completion
        """
        if not self.is_available():
            return self._get_error_response("OpenAI —Å–µ—Ä–≤–∏—Å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")

        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à –¥–ª—è –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
            cache_key = self._get_cache_key_for_messages(messages)
            cached_response = cache.get(cache_key)
            if cached_response:
                self.log_info("–û—Ç–≤–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ –∫—ç—à–∞")
                return cached_response

            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∑–∞–ø—Ä–æ—Å–∞
            params = {
                'model': kwargs.get('model', self.model),
                'messages': messages,
                'max_tokens': kwargs.get('max_tokens', self.max_tokens),
                'temperature': kwargs.get('temperature', self.temperature),
            }
            
            # Add tools if provided
            if 'tools' in kwargs and kwargs['tools']:
                params['tools'] = kwargs['tools']
                if 'tool_choice' in kwargs and kwargs['tool_choice']:
                    params['tool_choice'] = kwargs['tool_choice']
            
            # Add response_format if provided
            if 'response_format' in kwargs:
                params['response_format'] = kwargs['response_format']

            # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞
            try:
                response = self.client.chat.completions.create(**params)
            except Exception as api_error:
                error_msg = str(api_error)
                self.log_error(f"OpenAI API error: {error_msg}")
                
                # Check if it's the empty response error
                if "empty" in error_msg.lower() or "must contain either" in error_msg.lower():
                    # Return a simple text response as fallback
                    return {
                        'content': "–ü—Ä–∏–≤–µ—Ç! –ß–µ–º –º–æ–≥—É –ø–æ–º–æ—á—å? üåü",
                        'role': 'assistant',
                        'finish_reason': 'fallback',
                        'tokens_used': 0,
                        'model': params['model'],
                        'success': True,
                        'fallback': True
                    }
                
                # For other errors, return error response
                return self._get_error_response(f"API Error: {error_msg}")

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞
            result = self._process_response(response)

            # –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ—Ç tool_calls)
            if not result.get('tool_calls'):
                cache_timeout = getattr(settings, 'AI_RESPONSE_CACHE_TIMEOUT', 300)
                cache.set(cache_key, result, cache_timeout)

            self.log_info("Chat completion –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ", {
                'model': params['model'],
                'tokens_used': result.get('tokens_used', 0),
                'has_tool_calls': bool(result.get('tool_calls'))
            })

            return result

        except Exception as e:
            self.log_error(f"–û—à–∏–±–∫–∞ chat completion: {e}")
            return self._get_error_response(f"–û—à–∏–±–∫–∞ API: {str(e)}")

    def chat_completion_stream(self, messages: List[Dict[str, str]], **kwargs):
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç streaming –∑–∞–ø—Ä–æ—Å –∫ OpenAI Chat Completion
        """
        if not self.is_available():
            yield "OpenAI —Å–µ—Ä–≤–∏—Å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"
            return

        try:
            params = {
                'model': kwargs.get('model', self.model),
                'messages': messages,
                'max_tokens': kwargs.get('max_tokens', self.max_tokens),
                'temperature': kwargs.get('temperature', self.temperature),
                'stream': True
            }

            response = self.client.chat.completions.create(**params)

            for chunk in response:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content

        except Exception as e:
            self.log_error(f"–û—à–∏–±–∫–∞ chat completion stream: {e}")
            yield f"–û—à–∏–±–∫–∞: {str(e)}"

    def simple_completion(self, prompt: str, **kwargs) -> str:
        """
        –ü—Ä–æ—Å—Ç–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
        """
        messages = [{"role": "user", "content": prompt}]
        response = self.chat_completion(messages, **kwargs)
        return response.get('content', '')

    def is_available(self) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å OpenAI —Å–µ—Ä–≤–∏—Å–∞
        """
        return self.client is not None

    def get_models(self) -> List[str]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        """
        if not self.is_available():
            return []

        try:
            models = self.client.models.list()
            return [model.id for model in models.data]
        except Exception as e:
            self.log_error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π: {e}")
            return []

    def estimate_tokens(self, text: str) -> int:
        """
        –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ
        """
        # –ü—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞: ~4 —Å–∏–º–≤–æ–ª–∞ = 1 —Ç–æ–∫–µ–Ω
        return max(1, len(text) // 4)

    def truncate_messages(self, messages: List[Dict[str, str]], max_tokens: int = None) -> List[Dict[str, str]]:
        """
        –û–±—Ä–µ–∑–∞–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏—è —á—Ç–æ–±—ã —É–ª–æ–∂–∏—Ç—å—Å—è –≤ –ª–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤
        """
        if max_tokens is None:
            max_tokens = self.max_tokens * 2  # –ü—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞

        total_tokens = sum(self.estimate_tokens(msg.get('content', '')) for msg in messages)

        if total_tokens <= max_tokens:
            return messages

        # –û—Å—Ç–∞–≤–ª—è–µ–º —Å–∏—Å—Ç–µ–º–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –∏ –æ–±—Ä–µ–∑–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é
        system_messages = [msg for msg in messages if msg.get('role') == 'system']
        other_messages = [msg for msg in messages if msg.get('role') != 'system']

        # –û–±—Ä–µ–∑–∞–µ–º —Å –∫–æ–Ω—Ü–∞ (—Å–∞–º—ã–µ —Å—Ç–∞—Ä—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è)
        tokens_used = sum(self.estimate_tokens(msg.get('content', '')) for msg in system_messages)

        truncated_messages = system_messages.copy()
        for msg in reversed(other_messages):
            msg_tokens = self.estimate_tokens(msg.get('content', ''))
            if tokens_used + msg_tokens <= max_tokens:
                truncated_messages.insert(len(system_messages), msg)
                tokens_used += msg_tokens
            else:
                break

        self.log_info(f"–°–æ–æ–±—â–µ–Ω–∏—è –æ–±—Ä–µ–∑–∞–Ω—ã", {
            'original_count': len(messages),
            'truncated_count': len(truncated_messages),
            'tokens_saved': total_tokens - tokens_used
        })

        return truncated_messages

    def _process_response(self, response) -> Dict[str, Any]:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ—Ç–≤–µ—Ç –æ—Ç OpenAI
        """
        try:
            choice = response.choices[0]
            message = choice.message
            content = message.content or ""  # Handle None content when tool_calls present

            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤
            usage = getattr(response, 'usage', None)
            tokens_used = usage.total_tokens if usage else 0
            
            result = {
                'content': content.strip() if content else "",
                'role': message.role,
                'finish_reason': choice.finish_reason,
                'tokens_used': tokens_used,
                'model': response.model,
                'created': response.created,
                'success': True
            }
            
            # Add tool_calls if present
            if hasattr(message, 'tool_calls') and message.tool_calls:
                result['tool_calls'] = [
                    {
                        'id': tc.id,
                        'type': tc.type,
                        'function': {
                            'name': tc.function.name,
                            'arguments': tc.function.arguments
                        }
                    }
                    for tc in message.tool_calls
                ]

            return result

        except (IndexError, AttributeError) as e:
            self.log_error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—Ç–≤–µ—Ç–∞: {e}")
            return self._get_error_response("–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç –æ—Ç API")

    def _get_error_response(self, error_message: str) -> Dict[str, Any]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ç–≤–µ—Ç —Å –æ—à–∏–±–∫–æ–π
        """
        return {
            'content': '',
            'role': 'assistant',
            'finish_reason': 'error',
            'tokens_used': 0,
            'model': self.model,
            'error': error_message,
            'success': False
        }

    def _get_cache_key_for_messages(self, messages: List[Dict[str, str]]) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–ª—é—á –∫—ç—à–∞ –¥–ª—è —Å–æ–æ–±—â–µ–Ω–∏–π
        """
        import hashlib
        messages_str = json.dumps(messages, sort_keys=True)
        hash_obj = hashlib.md5(messages_str.encode())
        return f"openai_response_{hash_obj.hexdigest()}"

    def health_check(self) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Å–µ—Ä–≤–∏—Å–∞
        """
        if not self.is_available():
            return False

        try:
            # –ü—Ä–æ–±—É–µ–º –≤—ã–ø–æ–ª–Ω–∏—Ç—å –ø—Ä–æ—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å
            test_response = self.simple_completion("Say 'test'", max_tokens=5)
            return bool(test_response and 'test' in test_response.lower())
        except Exception as e:
            self.log_error(f"Health check –Ω–µ –ø—Ä–æ–π–¥–µ–Ω: {e}")
            return False

    def get_usage_stats(self) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è (–∑–∞–≥–ª—É—à–∫–∞)
        """
        # –í —Ä–µ–∞–ª—å–Ω–æ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ –∑–¥–µ—Å—å –º–æ–∂–Ω–æ –ø–æ–¥–∫–ª—é—á–∏—Ç—å API OpenAI –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        return {
            'model': self.model,
            'max_tokens': self.max_tokens,
            'temperature': self.temperature,
            'available': self.is_available()
        }

    def cleanup(self):
        """
        –û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤
        """
        if self.client:
            self.client.close()
            self.client = None
        self.log_info("OpenAI –∫–ª–∏–µ–Ω—Ç –æ—á–∏—â–µ–Ω")