"""
ðŸ§  Context Analyzer
Ð£Ñ‚Ð¸Ð»Ð¸Ñ‚Ð° Ð´Ð»Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð° ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð° ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹ Ð¸ Ð´Ð¸Ð°Ð»Ð¾Ð³Ð¾Ð²
"""

import re
import logging
from typing import Dict, List, Any, Tuple
from collections import Counter
logger = logging.getLogger(__name__)

try:
    import spacy
    SPACY_AVAILABLE = True
    try:
        nlp = spacy.load("ru_core_news_sm")
    except OSError:
        # Ð•ÑÐ»Ð¸ Ñ€ÑƒÑÑÐºÐ°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°, Ð¿Ð¾Ð¿Ñ€Ð¾Ð±ÑƒÐµÐ¼ Ð°Ð½Ð³Ð»Ð¸Ð¹ÑÐºÑƒÑŽ
        try:
            nlp = spacy.load("en_core_web_sm")
        except OSError:
            # Ð•ÑÐ»Ð¸ Ð¸ Ð°Ð½Ð³Ð»Ð¸Ð¹ÑÐºÐ°Ñ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð°, Ð¾Ñ‚ÐºÐ»ÑŽÑ‡Ð¸Ð¼ spacy
            nlp = None
            SPACY_AVAILABLE = False
            logger.warning("âš ï¸ spaCy Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ñ‹, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ Ð±Ð°Ð·Ð¾Ð²Ñ‹Ð¹ Ð°Ð½Ð°Ð»Ð¸Ð·")
except (ImportError, OSError):
    spacy = None
    nlp = None
    SPACY_AVAILABLE = False
    logger.warning("âš ï¸ spaCy Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ Ð±Ð°Ð·Ð¾Ð²Ñ‹Ð¹ Ð°Ð½Ð°Ð»Ð¸Ð·")


class ContextAnalyzer:
    """
    ðŸ” ÐÐ½Ð°Ð»Ð¸Ð·Ð°Ñ‚Ð¾Ñ€ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð° Ð´Ð»Ñ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ñ Ð¸Ð½Ñ‚ÐµÐ½Ñ‚Ð¾Ð², ÑÑƒÑ‰Ð½Ð¾ÑÑ‚ÐµÐ¹ Ð¸ Ñ‚Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸
    """

    def __init__(self):
        # ÐŸÐ°Ñ‚Ñ‚ÐµÑ€Ð½Ñ‹ Ð´Ð»Ñ Ð¸Ð½Ñ‚ÐµÐ½Ñ‚Ð¾Ð²
        self.intent_patterns = {
            'club_creation': [
                r'ÑÐ¾Ð·Ð´Ð°Ñ‚ÑŒ ÐºÐ»ÑƒÐ±', r'ÐºÐ°Ðº ÑÐ¾Ð·Ð´Ð°Ñ‚ÑŒ', r'Ð·Ð°Ñ€ÐµÐ³Ð¸ÑÑ‚Ñ€Ð¸Ñ€Ð¾Ð²Ð°Ñ‚ÑŒ ÐºÐ»ÑƒÐ±',
                r'Ð¾ÑÐ½Ð¾Ð²Ð°Ñ‚ÑŒ ÐºÐ»ÑƒÐ±', r'Ð¾Ñ‚ÐºÑ€Ñ‹Ñ‚ÑŒ ÐºÐ»ÑƒÐ±', r'Ð½Ð¾Ð²Ñ‹Ð¹ ÐºÐ»ÑƒÐ±'
            ],
            'club_joining': [
                r'Ð²ÑÑ‚ÑƒÐ¿Ð¸Ñ‚ÑŒ Ð² ÐºÐ»ÑƒÐ±', r'Ð¿Ñ€Ð¸ÑÐ¾ÐµÐ´Ð¸Ð½Ð¸Ñ‚ÑŒÑÑ', r'ÐºÐ°Ðº Ð²ÑÑ‚ÑƒÐ¿Ð¸Ñ‚ÑŒ',
                r'ÑƒÑ‡Ð°ÑÑ‚Ð¸Ðµ Ð² ÐºÐ»ÑƒÐ±Ðµ', r'ÑÑ‚Ð°Ñ‚ÑŒ ÑƒÑ‡Ð°ÑÑ‚Ð½Ð¸ÐºÐ¾Ð¼'
            ],
            'event_creation': [
                r'ÑÐ¾Ð·Ð´Ð°Ñ‚ÑŒ Ð¼ÐµÑ€Ð¾Ð¿Ñ€Ð¸ÑÑ‚Ð¸Ðµ', r'Ð¾Ñ€Ð³Ð°Ð½Ð¸Ð·Ð¾Ð²Ð°Ñ‚ÑŒ ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ðµ',
                r'Ð¿Ñ€Ð¾Ð²ÐµÑÑ‚Ð¸ Ñ„ÐµÑÑ‚Ð¸Ð²Ð°Ð»ÑŒ', r'Ð½Ð¾Ð²Ð¾Ðµ Ð¼ÐµÑ€Ð¾Ð¿Ñ€Ð¸ÑÑ‚Ð¸Ðµ'
            ],
            'technical_help': [
                r'Ð½Ðµ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚', r'Ð¾ÑˆÐ¸Ð±ÐºÐ°', r'Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ð°', r'Ð±Ð°Ð³',
                r'Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒ', r'Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶ÐºÐ°', r'Ñ‚ÐµÑ…Ð½Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ð²Ð¾Ð¿Ñ€Ð¾Ñ'
            ],
            'information_request': [
                r'Ñ‡Ñ‚Ð¾ Ñ‚Ð°ÐºÐ¾Ðµ', r'Ñ€Ð°ÑÑÐºÐ°Ð¶Ð¸ Ð¾', r'Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ Ð¾',
                r'ÐºÐ°Ðº Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚', r'Ð´Ð»Ñ Ñ‡ÐµÐ³Ð¾ Ð½ÑƒÐ¶ÐµÐ½'
            ],
            'recommendation': [
                r'Ð¿Ð¾ÑÐ¾Ð²ÐµÑ‚ÑƒÐ¹', r'Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´ÑƒÐ¹', r'ÐºÐ°ÐºÐ¾Ð¹ Ð²Ñ‹Ð±Ñ€Ð°Ñ‚ÑŒ',
                r'Ð»ÑƒÑ‡ÑˆÐ¸Ð¹ ÐºÐ»ÑƒÐ±', r'Ð¸Ð½Ñ‚ÐµÑ€ÐµÑÐ½Ñ‹Ðµ Ð¼ÐµÑ€Ð¾Ð¿Ñ€Ð¸ÑÑ‚Ð¸Ñ'
            ]
        }

        # Ð¡ÑƒÑ‰Ð½Ð¾ÑÑ‚Ð¸ Ð¸ ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ðµ ÑÐ»Ð¾Ð²Ð°
        self.entity_patterns = {
            'club_type': [
                r'ÑÐ¿Ð¾Ñ€Ñ‚Ð¸Ð²Ð½Ñ‹Ð¹ ÐºÐ»ÑƒÐ±', r'Ð¼ÑƒÐ·Ñ‹ÐºÐ°Ð»ÑŒÐ½Ñ‹Ð¹ ÐºÐ»ÑƒÐ±', r'ÐºÐ½Ð¸Ð¶Ð½Ñ‹Ð¹ ÐºÐ»ÑƒÐ±',
                r'Ð¸Ñ‚ ÐºÐ»ÑƒÐ±', r'Ñ‚Ð°Ð½Ñ†ÐµÐ²Ð°Ð»ÑŒÐ½Ñ‹Ð¹ ÐºÐ»ÑƒÐ±', r'Ñ…ÑƒÐ´Ð¾Ð¶ÐµÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ð¹ ÐºÐ»ÑƒÐ±'
            ],
            'event_type': [
                r'Ñ„ÐµÑÑ‚Ð¸Ð²Ð°Ð»ÑŒ', r'ÐºÐ¾Ð½Ñ„ÐµÑ€ÐµÐ½Ñ†Ð¸Ñ', r'ÑÐµÐ¼Ð¸Ð½Ð°Ñ€', r'Ð²Ð¾Ñ€ÐºÑˆÐ¾Ð¿',
                r'ÑÐ¾Ñ€ÐµÐ²Ð½Ð¾Ð²Ð°Ð½Ð¸Ðµ', r'ÐºÐ¾Ð½Ñ†ÐµÑ€Ñ‚', r'Ð²Ñ‹ÑÑ‚Ð°Ð²ÐºÐ°'
            ],
            'time_period': [
                r'Ð·Ð°Ð²Ñ‚Ñ€Ð°', r'ÑÐµÐ³Ð¾Ð´Ð½Ñ', r'Ð½Ð° ÑÐ»ÐµÐ´ÑƒÑŽÑ‰ÐµÐ¹ Ð½ÐµÐ´ÐµÐ»Ðµ', r'Ð² ÑÑ‚Ð¾Ð¼ Ð¼ÐµÑÑÑ†Ðµ',
                r'ÑÐºÐ¾Ñ€Ð¾', r'Ð±Ð»Ð¸Ð·ÐºÐ¾'
            ],
            'location': [
                r'Ð² Ð½Ð°ÑˆÐµÐ¼ Ð³Ð¾Ñ€Ð¾Ð´Ðµ', r'Ð¾Ð½Ð»Ð°Ð¹Ð½', r'Ð¾Ñ„Ð»Ð°Ð¹Ð½', r'Ð² Ñ†ÐµÐ½Ñ‚Ñ€Ðµ',
                r'Ð² ÑƒÐ½Ð¸Ð²ÐµÑ€ÑÐ¸Ñ‚ÐµÑ‚Ðµ'
            ]
        }

        # Sentiment ÑÐ»Ð¾Ð²Ð°Ñ€ÑŒ (ÑƒÐ¿Ñ€Ð¾Ñ‰ÐµÐ½Ð½Ñ‹Ð¹)
        self.positive_words = [
            'Ð¾Ñ‚Ð»Ð¸Ñ‡Ð½Ð¾', 'Ñ…Ð¾Ñ€Ð¾ÑˆÐ¾', 'Ð·Ð´Ð¾Ñ€Ð¾Ð²Ð¾', 'ÑÑƒÐ¿ÐµÑ€', 'ÐºÐ»Ð°ÑÑ', 'Ð·Ð°Ð¼ÐµÑ‡Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾',
            'ÑÐ¿Ð°ÑÐ¸Ð±Ð¾', 'Ð±Ð»Ð°Ð³Ð¾Ð´Ð°Ñ€ÑŽ', 'ÑƒÐ´Ð¾Ð±Ð½Ð¾', 'Ð¿Ð¾Ð½Ñ€Ð°Ð²Ð¸Ð»Ð¾ÑÑŒ'
        ]

        self.negative_words = [
            'Ð¿Ð»Ð¾Ñ…Ð¾', 'ÑƒÐ¶Ð°ÑÐ½Ð¾', 'Ð¾Ñ‚Ð²Ñ€Ð°Ñ‚Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾', 'Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ð°', 'Ð¾ÑˆÐ¸Ð±ÐºÐ°',
            'Ð½ÐµÑƒÐ´Ð¾Ð±Ð½Ð¾', 'ÑÐ»Ð¾Ð¶Ð½Ð¾', 'Ð½ÐµÐ¿Ð¾Ð½ÑÑ‚Ð½Ð¾', 'Ð½Ðµ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚', 'Ð±ÐµÑÐ¿Ð¾Ð»ÐµÐ·Ð½Ð¾'
        ]

    def analyze_message(self, text: str) -> Dict[str, Any]:
        """
        ðŸ” ÐŸÐ¾Ð»Ð½Ñ‹Ð¹ Ð°Ð½Ð°Ð»Ð¸Ð· ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ
        """
        try:
            analysis = {
                'original_text': text,
                'cleaned_text': self._clean_text(text),
                'intent': None,
                'confidence': 0.0,
                'entities': [],
                'sentiment': 'neutral',
                'sentiment_score': 0.0,
                'urgency': 'normal',
                'keywords': [],
                'language': self._detect_language(text),
                'complexity': self._assess_complexity(text)
            }

            # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ Ð¸Ð½Ñ‚ÐµÐ½Ñ‚Ð°
            intent_result = self._detect_intent(analysis['cleaned_text'])
            analysis['intent'] = intent_result['intent']
            analysis['confidence'] = intent_result['confidence']

            # Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ ÑÑƒÑ‰Ð½Ð¾ÑÑ‚ÐµÐ¹
            analysis['entities'] = self._extract_entities(analysis['cleaned_text'])

            # ÐÐ½Ð°Ð»Ð¸Ð· Ñ‚Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸
            sentiment_result = self._analyze_sentiment(analysis['cleaned_text'])
            analysis['sentiment'] = sentiment_result['sentiment']
            analysis['sentiment_score'] = sentiment_result['score']

            # ÐžÑ†ÐµÐ½ÐºÐ° ÑÑ€Ð¾Ñ‡Ð½Ð¾ÑÑ‚Ð¸
            analysis['urgency'] = self._assess_urgency(analysis['cleaned_text'])

            # Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ñ… ÑÐ»Ð¾Ð²
            analysis['keywords'] = self._extract_keywords(analysis['cleaned_text'])

            return analysis

        except Exception as e:
            logger.error(f"âŒ Error analyzing message: {e}")
            return {
                'original_text': text,
                'intent': 'general',
                'confidence': 0.0,
                'entities': [],
                'sentiment': 'neutral',
                'error': str(e)
            }

    def _clean_text(self, text: str) -> str:
        """ÐžÑ‡Ð¸ÑÑ‚ÐºÐ° Ñ‚ÐµÐºÑÑ‚Ð° Ð¾Ñ‚ Ð»Ð¸ÑˆÐ½Ð¸Ñ… ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð² Ð¸ Ð½Ð¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ"""
        # Ð£Ð´Ð°Ð»ÐµÐ½Ð¸Ðµ Ð»Ð¸ÑˆÐ½Ð¸Ñ… Ð¿Ñ€Ð¾Ð±ÐµÐ»Ð¾Ð² Ð¸ Ð¿ÐµÑ€ÐµÐ½Ð¾ÑÐ¾Ð²
        text = re.sub(r'\s+', ' ', text.strip())

        # ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¿ÑƒÐ½ÐºÑ‚ÑƒÐ°Ñ†Ð¸Ð¸
        text = re.sub(r'[^\w\s\?\!\.\,\-]', ' ', text)

        return text.lower()

    def _detect_intent(self, text: str) -> Dict[str, Any]:
        """ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ Ð¾ÑÐ½Ð¾Ð²Ð½Ð¾Ð³Ð¾ Ð¸Ð½Ñ‚ÐµÐ½Ñ‚Ð° ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ"""
        intent_scores = {}

        # ÐŸÑ€Ð¾Ð²ÐµÑ€ÐºÐ° ÐºÐ°Ð¶Ð´Ð¾Ð³Ð¾ Ð¿Ð°Ñ‚Ñ‚ÐµÑ€Ð½Ð° Ð¸Ð½Ñ‚ÐµÐ½Ñ‚Ð°
        for intent, patterns in self.intent_patterns.items():
            score = 0
            matched_patterns = []

            for pattern in patterns:
                matches = len(re.findall(pattern, text, re.IGNORECASE))
                if matches > 0:
                    score += matches
                    matched_patterns.append(pattern)

            if score > 0:
                intent_scores[intent] = {
                    'score': score,
                    'patterns': matched_patterns
                }

        # Ð’Ñ‹Ð±Ð¾Ñ€ Ð»ÑƒÑ‡ÑˆÐµÐ³Ð¾ Ð¸Ð½Ñ‚ÐµÐ½Ñ‚Ð°
        if intent_scores:
            best_intent = max(intent_scores.items(), key=lambda x: x[1]['score'])
            confidence = min(best_intent[1]['score'] / 3.0, 1.0)  # ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ
            return {
                'intent': best_intent[0],
                'confidence': confidence,
                'matched_patterns': best_intent[1]['patterns']
            }

        return {
            'intent': 'general',
            'confidence': 0.1,
            'matched_patterns': []
        }

    def _extract_entities(self, text: str) -> List[Dict[str, Any]]:
        """Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð¸Ð¼ÐµÐ½Ð¾Ð²Ð°Ð½Ð½Ñ‹Ñ… ÑÑƒÑ‰Ð½Ð¾ÑÑ‚ÐµÐ¹"""
        entities = []

        # ÐŸÐ¾Ð¸ÑÐº Ð¿Ð¾ Ð¿Ð°Ñ‚Ñ‚ÐµÑ€Ð½Ð°Ð¼
        for entity_type, patterns in self.entity_patterns.items():
            for pattern in patterns:
                matches = re.finditer(pattern, text, re.IGNORECASE)
                for match in matches:
                    entities.append({
                        'type': entity_type,
                        'value': match.group(),
                        'start': match.start(),
                        'end': match.end(),
                        'confidence': 0.8
                    })

        # Ð•ÑÐ»Ð¸ Ð´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½ spaCy, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ ÐµÐ³Ð¾ Ð´Ð»Ñ NER
        if SPACY_AVAILABLE and nlp:
            try:
                doc = nlp(text)
                for ent in doc.ents:
                    entity_type = self._map_spacy_label(ent.label_)
                    if entity_type:
                        entities.append({
                            'type': entity_type,
                            'value': ent.text,
                            'start': ent.start_char,
                            'end': ent.end_char,
                            'confidence': 0.9
                        })
            except Exception as e:
                logger.warning(f"âš ï¸ ÐžÑˆÐ¸Ð±ÐºÐ° spaCy NER: {e}")

        # Ð£Ð´Ð°Ð»ÐµÐ½Ð¸Ðµ Ð´ÑƒÐ±Ð»Ð¸ÐºÐ°Ñ‚Ð¾Ð²
        unique_entities = []
        seen_values = set()

        for entity in entities:
            key = (entity['type'], entity['value'].lower())
            if key not in seen_values:
                seen_values.add(key)
                unique_entities.append(entity)

        return unique_entities[:10]  # ÐžÐ³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ðµ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð° ÑÑƒÑ‰Ð½Ð¾ÑÑ‚ÐµÐ¹

    def _map_spacy_label(self, spacy_label: str) -> str:
        """ÐœÐ°Ð¿Ð¿Ð¸Ð½Ð³ Ð¼ÐµÑ‚Ð¾Ðº spaCy Ð½Ð° Ð½Ð°ÑˆÐ¸ Ñ‚Ð¸Ð¿Ñ‹ ÑÑƒÑ‰Ð½Ð¾ÑÑ‚ÐµÐ¹"""
        label_mapping = {
            'ORG': 'organization',
            'PERSON': 'person',
            'LOC': 'location',
            'GPE': 'location',
            'DATE': 'date',
            'TIME': 'time',
            'MONEY': 'money',
            'PRODUCT': 'product'
        }
        return label_mapping.get(spacy_label)

    def _analyze_sentiment(self, text: str) -> Dict[str, Any]:
        """ÐÐ½Ð°Ð»Ð¸Ð· Ñ‚Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸ Ñ‚ÐµÐºÑÑ‚Ð°"""
        words = text.split()

        positive_count = sum(1 for word in words if word in self.positive_words)
        negative_count = sum(1 for word in words if word in self.negative_words)

        total_words = len(words)
        if total_words == 0:
            return {'sentiment': 'neutral', 'score': 0.0}

        # Ð Ð°ÑÑ‡ÐµÑ‚ Ð¾Ñ†ÐµÐ½ÐºÐ¸
        score = (positive_count - negative_count) / total_words

        # ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ Ñ‚Ð¾Ð½Ð°Ð»ÑŒÐ½Ð¾ÑÑ‚Ð¸
        if score > 0.1:
            sentiment = 'positive'
        elif score < -0.1:
            sentiment = 'negative'
        else:
            sentiment = 'neutral'

        return {
            'sentiment': sentiment,
            'score': score,
            'positive_words': positive_count,
            'negative_words': negative_count
        }

    def _assess_urgency(self, text: str) -> str:
        """ÐžÑ†ÐµÐ½ÐºÐ° ÑÑ€Ð¾Ñ‡Ð½Ð¾ÑÑ‚Ð¸ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ"""
        urgency_patterns = [
            r'ÑÑ€Ð¾Ñ‡Ð½Ð¾', r'Ð½ÐµÐ¼ÐµÐ´Ð»ÐµÐ½Ð½Ð¾', r'Ð¿Ð¾Ð¶Ð°Ð»ÑƒÐ¹ÑÑ‚Ð° Ð¿Ð¾Ð¼Ð¾Ð³Ð¸Ñ‚Ðµ', r'Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ð°',
            r'Ð½Ðµ Ñ€Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚', r'Ð¾ÑˆÐ¸Ð±ÐºÐ°', r'urgent', r'asap', r'immediately'
        ]

        urgency_score = sum(1 for pattern in urgency_patterns
                           if re.search(pattern, text, re.IGNORECASE))

        if urgency_score >= 2:
            return 'high'
        elif urgency_score >= 1:
            return 'medium'
        else:
            return 'normal'

    def _extract_keywords(self, text: str) -> List[str]:
        """Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ñ… ÑÐ»Ð¾Ð²"""
        # ÐŸÑ€Ð¾ÑÑ‚Ð°Ñ Ñ€ÐµÐ°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ - Ð¼Ð¾Ð¶Ð½Ð¾ ÑƒÐ»ÑƒÑ‡ÑˆÐ¸Ñ‚ÑŒ Ñ Ð¿Ð¾Ð¼Ð¾Ñ‰ÑŒÑŽ TF-IDF Ð¸Ð»Ð¸ RAKE
        stop_words = {
            'Ð¸', 'Ð²', 'Ð½Ð°', 'Ñ', 'Ð¿Ð¾', 'Ð´Ð»Ñ', 'Ð¾', 'Ð¾Ð±', 'Ð¾Ñ‚', 'Ð´Ð¾', 'Ñƒ', 'Ðº', 'Ð¸Ð·',
            'Ñ‡Ñ‚Ð¾', 'ÐºÐ°Ðº', 'Ð³Ð´Ðµ', 'ÐºÐ¾Ð³Ð´Ð°', 'Ð¿Ð¾Ñ‡ÐµÐ¼Ñƒ', 'Ð·Ð°Ñ‡ÐµÐ¼', 'ÐºÑ‚Ð¾', 'Ñ‡ÐµÐ¹',
            'ÑÑ‚Ð¾', 'Ñ‚Ð¾Ñ‚', 'Ñ‚Ð¾Ñ‚', 'Ñ‚Ð°ÐºÐ¾Ð¹', 'Ñ‚Ð°ÐºÐ¸Ð¼', 'Ñ‚Ð°ÐºÐ°Ñ', 'Ñ‚Ð°ÐºÐ¾Ðµ',
            'Ð±Ñ‹Ñ‚ÑŒ', 'Ð±Ñ‹Ð»', 'Ð±Ñ‹Ð»Ð°', 'Ð±Ñ‹Ð»Ð¾', 'Ð±ÑƒÐ´ÐµÑ‚', 'ÐµÑÑ‚ÑŒ', 'ÑÐ²Ð»ÑÑ‚ÑŒÑÑ',
            'Ñ', 'Ñ‚Ñ‹', 'Ð¾Ð½', 'Ð¾Ð½Ð°', 'Ð¾Ð½Ð¾', 'Ð¼Ñ‹', 'Ð²Ñ‹', 'Ð¾Ð½Ð¸',
            'Ð¼Ð¾Ð¹', 'Ñ‚Ð²Ð¾Ð¹', 'ÐµÐ³Ð¾', 'ÐµÑ‘', 'Ð½Ð°Ñˆ', 'Ð²Ð°Ñˆ', 'Ð¸Ñ…',
            'ÑÐ²Ð¾Ð¹', 'ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ð¹', 'ÐºÐ¾Ñ‚Ð¾Ñ€Ð°Ñ', 'ÐºÐ¾Ñ‚Ð¾Ñ€Ð¾Ðµ', 'ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ'
        }

        words = re.findall(r'\b\w+\b', text.lower())
        keywords = [word for word in words if word not in stop_words and len(word) > 2]

        # ÐŸÐ¾Ð´ÑÑ‡ÐµÑ‚ Ñ‡Ð°ÑÑ‚Ð¾Ñ‚Ð½Ð¾ÑÑ‚Ð¸
        word_freq = Counter(keywords)
        return [word for word, count in word_freq.most_common(10)]

    def _detect_language(self, text: str) -> str:
        """ÐžÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ ÑÐ·Ñ‹ÐºÐ° Ñ‚ÐµÐºÑÑ‚Ð°"""
        # ÐŸÑ€Ð¾ÑÑ‚Ð°Ñ ÑÐ²Ñ€Ð¸ÑÑ‚Ð¸ÐºÐ° Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²
        russian_chars = set('Ð°Ð±Ð²Ð³Ð´ÐµÑ‘Ð¶Ð·Ð¸Ð¹ÐºÐ»Ð¼Ð½Ð¾Ð¿Ñ€ÑÑ‚ÑƒÑ„Ñ…Ñ†Ñ‡ÑˆÑ‰ÑŠÑ‹ÑŒÑÑŽÑ')
        text_chars = set(text.lower())

        if len(text_chars & russian_chars) > len(text_chars) * 0.3:
            return 'russian'
        else:
            return 'english'  # ÐŸÐ¾ ÑƒÐ¼Ð¾Ð»Ñ‡Ð°Ð½Ð¸ÑŽ

    def _assess_complexity(self, text: str) -> str:
        """ÐžÑ†ÐµÐ½ÐºÐ° ÑÐ»Ð¾Ð¶Ð½Ð¾ÑÑ‚Ð¸ Ñ‚ÐµÐºÑÑ‚Ð°"""
        words = text.split()
        sentences = re.split(r'[.!?]+', text)

        avg_sentence_length = len(words) / max(len(sentences), 1)
        word_length = sum(len(word) for word in words) / max(len(words), 1)

        # Ð”Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ñ„Ð°ÐºÑ‚Ð¾Ñ€Ñ‹ ÑÐ»Ð¾Ð¶Ð½Ð¾ÑÑ‚Ð¸
        has_numbers = bool(re.search(r'\d+', text))
        has_questions = text.count('?') > 0
        has_multiple_sentences = len(sentences) > 2

        complexity_score = 0
        if avg_sentence_length > 15:
            complexity_score += 1
        if word_length > 6:
            complexity_score += 1
        if has_numbers:
            complexity_score += 0.5
        if has_questions:
            complexity_score += 0.5
        if has_multiple_sentences:
            complexity_score += 1

        if complexity_score >= 2.5:
            return 'high'
        elif complexity_score >= 1:
            return 'medium'
        else:
            return 'low'

    def analyze_conversation_context(self, messages: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        ðŸ” ÐÐ½Ð°Ð»Ð¸Ð· ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð° Ð²ÑÐµÐ³Ð¾ Ð´Ð¸Ð°Ð»Ð¾Ð³Ð°
        """
        if not messages:
            return {'summary': 'No messages to analyze'}

        try:
            context_analysis = {
                'total_messages': len(messages),
                'user_messages': sum(1 for msg in messages if msg.get('role') == 'user'),
                'assistant_messages': sum(1 for msg in messages if msg.get('role') == 'assistant'),
                'avg_message_length': 0,
                'main_topics': [],
                'conversation_flow': 'normal',
                'user_engagement': 'medium',
                'progress_indicators': []
            }

            # Ð¡Ñ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸ÐºÐ° Ð¿Ð¾ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸ÑÐ¼
            total_length = sum(len(msg.get('content', '')) for msg in messages)
            context_analysis['avg_message_length'] = total_length / len(messages)

            # ÐÐ½Ð°Ð»Ð¸Ð· Ñ‚ÐµÐ¼
            all_keywords = []
            for msg in messages:
                if msg.get('content'):
                    analysis = self.analyze_message(msg['content'])
                    all_keywords.extend(analysis['keywords'])

            # Ð¢Ð¾Ð¿-5 ÐºÐ»ÑŽÑ‡ÐµÐ²Ñ‹Ñ… ÑÐ»Ð¾Ð² Ð´Ð¸Ð°Ð»Ð¾Ð³Ð°
            keyword_freq = Counter(all_keywords)
            context_analysis['main_topics'] = [word for word, count in keyword_freq.most_common(5)]

            # ÐÐ½Ð°Ð»Ð¸Ð· Ð¿Ð¾Ñ‚Ð¾ÐºÐ° Ð´Ð¸Ð°Ð»Ð¾Ð³Ð°
            if context_analysis['user_messages'] > context_analysis['assistant_messages'] * 2:
                context_analysis['conversation_flow'] = 'user_dominant'
            elif context_analysis['assistant_messages'] > context_analysis['user_messages'] * 2:
                context_analysis['conversation_flow'] = 'assistant_dominant'

            # ÐžÑ†ÐµÐ½ÐºÐ° Ð²Ð¾Ð²Ð»ÐµÑ‡ÐµÐ½Ð½Ð¾ÑÑ‚Ð¸
            avg_response_length = 0
            user_lengths = [len(msg.get('content', '')) for msg in messages if msg.get('role') == 'user']

            if user_lengths:
                avg_response_length = sum(user_lengths) / len(user_lengths)
                if avg_response_length > 100:
                    context_analysis['user_engagement'] = 'high'
                elif avg_response_length < 30:
                    context_analysis['user_engagement'] = 'low'

            return context_analysis

        except Exception as e:
            logger.error(f"âŒ Error analyzing conversation context: {e}")
            return {'error': str(e)}